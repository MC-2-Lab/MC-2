{"0": "<li class=\"content\">\n    <h3>Evaluation_VR</h3>\n    A subjective evaluation tool on VR video with GUI in MATLAB. SSCQS and MSCQS method are provided for different aims.\n    <br/>|\n    <a href='https://github.com/Archer-Tatsu/Evaluation_VR-onebar-vive'>GitHub</a>\n    |\n</li>", "1": "<li class=\"content\">\n    <h3>Panoramic video head tracking data</h3>\n    HTC Vive head tracking data of 40 subjects on 48 panoramic video sequences in 8 classes.\n    <br/>|\n    <a href='https://github.com/Archer-Tatsu/head-tracking'>GitHub</a>\n    |\n</li>\n", "2": "<li class=\"content\">\n    <h3>NCP- and CP-PSNR</h3>\n    Perceptual objective quality assessment methods of omnidirectional video.\n    <br/>|\n    <a href = \"https://github.com/Archer-Tatsu/CP-PSNR\">GitHub</a>\n    |\n</li>\n", "3": "<li class=\"content\">\n    <h3>V-CNN</h3>\n    Viewport-based CNN for visual quality assessment on 360\u00b0 video.\n    <br/>|\n    <a href = \"https://github.com/Archer-Tatsu/V-CNN\">GitHub</a>\n    |\n</li>\n", "4": "<li class=\"content\">\n    <h3>PVS-HMEM database</h3>\n    PVS-HMEM (Panoramic Video Sequences with Head Movement & Eye Movement database) database contains both Head Movement and Eye Movement data of 58 subjects on 76 panoramic videos.\n    <br/>|\n    <a href='https://github.com/YuhangSong/DHP'>GitHub</a>\n    |\n</li>\n", "5": "</ul>\n<h2>Virtual/Augmented Reality</h2>\n<hr/>\n<ul>\n<li class=\"content\">\n    <h3>VQA-ODV</h3>\n    A large-scale VQA dataset of omnidirectional video proposed in ACMMM 2018 paper.\n    <br/>|\n    <a href='https://github.com/Archer-Tatsu/VQA-ODV'>GitHub</a>\n    |\n</li>\n", "6": "<li class=\"content\">\n    <h3>Weight-based R-lambda RC</h3>\n    The core code for the weight-based R-lambda rate control\n    <br/>|\n    <a href='https://sites.google.com/site/lsxweb/Home/publication/spic2015source'>Google Sites</a>\n    |\n</li>\n", "7": "<li class=\"content\">\n    <h3>RTE core</h3>\n    The core code for the recursive Taylor expansion (RTE) method\n    <br/>|\n    <a href='https://sites.google.com/site/lsxweb/Home/publication/tcsvt2016source'>Google Sites</a>\n    |\n</li>\n", "8": "<li class=\"content\">\n    <h3>DS-CNN</h3>\n    The Caffe code of the decoder-side scalable convolutional neural network (DS-CNN)\n    <br/>|\n    <a href='https://github.com/ryangBUAA/DS-CNN'>GitHub</a>\n    |\n</li>\n", "9": "<li class=\"content\">\n    <h3>Database for CU partition of HEVC (CPH)</h3>\n    A large-scale database for CU partition of HEVC, to reduce encoding complexity through deep learning based approach.\n    <br/>|\n    <a href='https://github.com/tianyili2017/CPH'>GitHub (Database)</a>\n    |\n    <a href='https://github.com/tianyili2017/HEVC-Complexity-Reduction'>GitHub (Code)</a>\n    |\n</li>\n", "10": "<li class=\"content\">\n    <h3>Eye-tracking database for 40 typical images</h3>\n    An eye-tracking database for 40 typical images.\n    <br/>|\n    <a href='https://drive.google.com/drive/folders/0ByJ_K4vlqqQLTDg4R29QY3F0Qzg'>Google Drive</a>\n    |\n</li>\n", "11": "<li class=\"content\">\n    <h3>Subjective-driven complexity control approach for HEVC</h3>\n    Subjective-driven Complexity Control Source Code.\n    <br/>|\n    <a href='https://github.com/cindydeng1991/DENG_TCSVT/'>GitHub</a>\n    |\n</li>\n", "12": "<li class=\"content\">\n    <h3>Database for HEVC in-loop filter</h3>\n    A large-scale database for HEVC in-loop filter (HIF).\n    <br/>|\n    <a href='https://github.com/tianyili2017/HIF-Database'>GitHub</a> |\n</li>\n", "13": "    |\n</li>\t\t\t\t\n", "14": "</ul>\n<h2>Video Compression</h2>\n<hr/>\n<ul>\n<li class=\"content\">\n    <h3>MFQE 2.0</h3>\n    The TensorFlow Code for testing multi-frame quality enhancement (MFQE) approach v2.0.\n    <br/>|\n    <a href='https://github.com/RyanXingQL/MFQEv2.0'>GitHub</a>\n    |\n    <a href='https://bhpan.buaa.edu.cn/link/0EBF3709E3168E9A78206391258715A3'>\u5317\u822a\u4e91\u76d8</a>\n", "15": "<li class=\"content\">\n    <h3>Face videos for saliency detection</h3>\n    Face videos for saliency detection.\n    <br/>|\n    <a href='https://github.com/RenYun2016/-Particle-Filter-for-Dynamic-GMM'>GitHub</a>\n    |\n</li>\n", "16": "<li class=\"content\">\n    <h3>Face images for saliency detection</h3>\n    510 Face images for saliency detection.\n    <br/>|\n    <a href='https://github.com/RenYun2016/Face'>GitHub</a>\n    |\n</li>\n", "17": "<li class=\"content\">\n    <h3>Natural-scene images memorability (NSIM) database</h3>\n    NSIM dataset includes 258 natura-scene images and their memorability scores.\n    <br/>|\n    <a href='https://github.com/Blossomsblue/Memorability'>GitHub</a>\n    |\n</li>\n", "18": "<li class=\"content\">\n    <h3>Predicting salient face in multiple-face videos</h3>\n    Eye tracking data on multiple face videos.\n    <br/>|\n    <a href='https://github.com/yufanLIU/salient-face-in-MUVFET'>GitHub</a>\n    |\n</li>\n", "19": "<li class=\"content\">\n    <h3>Large-scale eye-tracking database of videos (LEDOV)</h3>\n    LEDOV includes\u00a0538 videos\u00a0with diverse content, containing a total of\u00a0179,336 frames\u00a0and\u00a06,431 seconds. The diverse content refers to the daily action, sports, social activity and art performance of human, and the videos of animal and man-man objects are also included. Moreover,\u00a032 participants\u00a0(18 males and 14 females), aging from 20 to 56 (32 on average), were recruited to participate in the eye-tracking experiment.\n    <br/>|\n    <a href='https://github.com/remega/LEDOV-eye-tracking-database'>GitHub</a>\n    |\n</li>\n", "20": "<li class=\"content\">\n    <h3>Raw video dataset</h3>\n    33 raw videos from the latest\u00a0HEVC standard test sets, such as JCT-VC, which have been commonly utilized for evaluating HEVC performance. \u00a0The eye fixations of all 32 subjects over each video frame were recorded by a Tobii TX300 eye tracker at a sample rate of 300 Hz.\n    <br/>|\n    <a href='https://github.com/remega/video_database'>GitHub</a>\n    |\n</li>\n", "21": "</ul>\n<h2>Saliency Detection</h2>\n<hr/>\n<ul>\n<li class=\"content\">\n    <h3>RSRCNN</h3>\n    Road structure refined CNN (RSRCNN)\n    <br/>|\n    <a href='https://github.com/yananweinbaa/RSRCNN'>GitHub</a>\n    |\n</li>\n"}